import torch
import sys

print("="*60)
print("GPU ç’°å¢ƒæª¢æŸ¥")
print("="*60)

# æª¢æŸ¥ PyTorch ç‰ˆæœ¬
print(f"\nğŸ“¦ PyTorch ç‰ˆæœ¬: {torch.__version__}")
print(f"ğŸ“¦ CUDA ç‰ˆæœ¬: {torch.version.cuda}")

# æª¢æŸ¥ CUDA æ˜¯å¦å¯ç”¨
cuda_available = torch.cuda.is_available()
print(f"\nğŸ® CUDA å¯ç”¨: {cuda_available}")

if cuda_available:
    # GPU è³‡è¨Š
    gpu_count = torch.cuda.device_count()
    print(f"ğŸ® GPU æ•¸é‡: {gpu_count}")
    
    for i in range(gpu_count):
        print(f"\nğŸ® GPU {i}:")
        print(f"   åç¨±: {torch.cuda.get_device_name(i)}")
        print(f"   è¨˜æ†¶é«”ç¸½é‡: {torch.cuda.get_device_properties(i).total_memory / 1024**3:.2f} GB")
        
        # æª¢æŸ¥ç•¶å‰è¨˜æ†¶é«”ä½¿ç”¨
        if hasattr(torch.cuda, 'mem_get_info'):
            free, total = torch.cuda.mem_get_info(i)
            print(f"   å¯ç”¨è¨˜æ†¶é«”: {free / 1024**3:.2f} GB")
            print(f"   å·²ç”¨è¨˜æ†¶é«”: {(total - free) / 1024**3:.2f} GB")
    
    # æ¸¬è©¦ç°¡å–®é‹ç®—
    print(f"\nğŸ§ª æ¸¬è©¦ GPU é‹ç®—...")
    try:
        x = torch.randn(1000, 1000).cuda()
        y = torch.randn(1000, 1000).cuda()
        z = torch.matmul(x, y)
        print(f"âœ“ GPU é‹ç®—æ¸¬è©¦æˆåŠŸï¼")
        print(f"âœ“ ç•¶å‰ä½¿ç”¨çš„ GPU: {torch.cuda.current_device()}")
    except Exception as e:
        print(f"âŒ GPU é‹ç®—æ¸¬è©¦å¤±æ•—: {e}")
else:
    print("\nâŒ CUDA ä¸å¯ç”¨ï¼Œå°‡ä½¿ç”¨ CPU è¨“ç·´")
    print("\nè«‹æª¢æŸ¥:")
    print("1. æ˜¯å¦å®‰è£äº†æ”¯æ´ CUDA çš„ PyTorch")
    print("2. NVIDIA é©…å‹•æ˜¯å¦æ­£ç¢ºå®‰è£")
    print("3. CUDA toolkit æ˜¯å¦å®‰è£")

print("\n" + "="*60)