import torch
import pandas as pd
from torch.utils.data import DataLoader
from tqdm import tqdm
import numpy as np
import os
from dataset import CXRDataset, get_valid_transforms, get_tta_transforms


def predict_with_tta(model, test_loader, tta_transforms, device, config):
    """ä½¿ç”¨ TTA é€²è¡Œé æ¸¬"""
    model.eval()
    
    all_predictions = []
    all_filenames = []
    
    with torch.no_grad():
        for images, filenames in tqdm(test_loader, desc='Predicting with TTA'):
            batch_preds = []
            
            # å°æ¯ç¨® TTA è½‰æ›é€²è¡Œé æ¸¬
            for tta_transform in tta_transforms:
                images_tta = images.to(device)
                outputs = model(images_tta)
                probs = torch.softmax(outputs, dim=1)
                batch_preds.append(probs.cpu().numpy())
            
            # å¹³å‡æ‰€æœ‰ TTA é æ¸¬
            avg_preds = np.mean(batch_preds, axis=0)
            all_predictions.append(avg_preds)
            all_filenames.extend(filenames)
    
    all_predictions = np.concatenate(all_predictions, axis=0)
    return all_predictions, all_filenames


def predict(model, test_loader, device):
    """æ¨™æº–é æ¸¬"""
    model.eval()
    
    all_predictions = []
    all_filenames = []
    
    with torch.no_grad():
        for images, filenames in tqdm(test_loader, desc='Predicting'):
            images = images.to(device)
            outputs = model(images)
            probs = torch.softmax(outputs, dim=1)
            
            all_predictions.append(probs.cpu().numpy())
            all_filenames.extend(filenames)
    
    all_predictions = np.concatenate(all_predictions, axis=0)
    return all_predictions, all_filenames


def create_submission(config, model_path=None):
    """å‰µå»ºæäº¤æª”æ¡ˆ"""
    from model import create_model
    
    # è¼‰å…¥æ¨¡å‹
    model = create_model(config)
    if model_path:
        checkpoint = torch.load(model_path, map_location=config.DEVICE)
        model.load_state_dict(checkpoint['model_state_dict'])
        print(f"âœ“ è¼‰å…¥æ¨¡å‹: {model_path}")
        print(f"  Epoch: {checkpoint['epoch']}, F1: {checkpoint['best_f1']:.4f}")
    model.to(config.DEVICE)
    model.eval()
    
    # è®€å–æˆ–å»ºç«‹æ¸¬è©¦è³‡æ–™ CSV
    if os.path.exists(config.TEST_SAMPLE_CSV):
        print(f"âœ“ æ‰¾åˆ°æ¸¬è©¦é›† CSV: {config.TEST_SAMPLE_CSV}")
        test_df = pd.read_csv(config.TEST_SAMPLE_CSV)
    else:
        print(f"âš ï¸  æœªæ‰¾åˆ° {config.TEST_SAMPLE_CSV}ï¼Œè‡ªå‹•å»ºç«‹...")
        
        # å¾ç›®éŒ„å»ºç«‹æ¸¬è©¦é›† CSV
        if not os.path.exists(config.TEST_IMG_DIR):
            raise FileNotFoundError(f"æ¸¬è©¦é›†ç›®éŒ„ä¸å­˜åœ¨: {config.TEST_IMG_DIR}")
        
        test_images = []
        for f in os.listdir(config.TEST_IMG_DIR):
            if f.lower().endswith(('.jpg', '.jpeg', '.png')):
                test_images.append(f)
        
        test_images.sort()
        
        test_df = pd.DataFrame({
            'new_filename': test_images,
            'normal': 0,
            'bacteria': 0,
            'virus': 0,
            'COVID-19': 0
        })
        
        # å„²å­˜ä»¥å‚™å¾Œç”¨
        test_df.to_csv(config.TEST_SAMPLE_CSV, index=False)
        print(f"âœ“ å»ºç«‹æ¸¬è©¦é›† CSV: {config.TEST_SAMPLE_CSV} ({len(test_df)} å¼µ)")
    
    print(f"æ¸¬è©¦é›†åœ–ç‰‡æ•¸é‡: {len(test_df)}")
    
    # å‰µå»ºæ¸¬è©¦é›†
    test_dataset = CXRDataset(
        test_df,
        config.TEST_IMG_DIR,
        transform=get_valid_transforms(config.IMG_SIZE),
        is_test=True
    )
    
    test_loader = DataLoader(
        test_dataset,
        batch_size=config.BATCH_SIZE,
        shuffle=False,
        num_workers=0,  # Windows å»ºè­°ç”¨ 0
        pin_memory=True
    )
    
    # é æ¸¬
    if config.USE_TTA:
        print("ğŸ”„ ä½¿ç”¨ TTA é æ¸¬...")
        tta_transforms = get_tta_transforms(config.IMG_SIZE)
        predictions, filenames = predict_with_tta(
            model, test_loader, tta_transforms, config.DEVICE, config
        )
    else:
        print("ğŸ”„ æ¨™æº–é æ¸¬...")
        predictions, filenames = predict(model, test_loader, config.DEVICE)
    
    # è½‰æ›ç‚º one-hot æ ¼å¼
    pred_labels = np.argmax(predictions, axis=1)
    one_hot = np.zeros((len(pred_labels), config.NUM_CLASSES), dtype=int)
    one_hot[np.arange(len(pred_labels)), pred_labels] = 1
    
    # å‰µå»ºæäº¤ DataFrame
    submission_df = pd.DataFrame({
        'new_filename': filenames,
        'normal': one_hot[:, 0],
        'bacteria': one_hot[:, 1],
        'virus': one_hot[:, 2],
        'COVID-19': one_hot[:, 3]
    })
    
    # ç¢ºä¿é †åºèˆ‡åŸå§‹æ¸¬è©¦æª”æ¡ˆä¸€è‡´
    submission_df = test_df[['new_filename']].merge(
        submission_df, on='new_filename', how='left'
    )
    
    # è™•ç†å¯èƒ½çš„ NaNï¼ˆå¦‚æœæœ‰åœ–ç‰‡æ²’æœ‰è¢«é æ¸¬åˆ°ï¼‰
    submission_df.fillna(0, inplace=True)
    submission_df[['normal', 'bacteria', 'virus', 'COVID-19']] = \
        submission_df[['normal', 'bacteria', 'virus', 'COVID-19']].astype(int)
    
    # å„²å­˜ CSV
    submission_df.to_csv(config.OUTPUT_CSV, index=False)
    print(f"\nâœ“ æäº¤æª”æ¡ˆå·²å„²å­˜: {config.OUTPUT_CSV}")
    print(f"   Shape: {submission_df.shape}")
    
    # é¡¯ç¤ºé æ¸¬åˆ†ä½ˆ
    print(f"\nğŸ“Š é æ¸¬çµæœåˆ†ä½ˆ:")
    for col in config.CLASS_NAMES:
        count = submission_df[col].sum()
        print(f"  {col:12s}: {count:4d} ({count/len(submission_df)*100:5.1f}%)")
    
    print(f"\nå‰ 10 ç­†é æ¸¬:")
    print(submission_df.head(10))
    
    # é©—è­‰æ ¼å¼
    print(f"\nğŸ” é©—è­‰æäº¤æ ¼å¼...")
    assert submission_df.shape[1] == 5, f"æ‡‰è©²æœ‰ 5 æ¬„ï¼Œå¯¦éš›: {submission_df.shape[1]}"
    assert all(submission_df[config.CLASS_NAMES].sum(axis=1) == 1), "æ¯åˆ—æ‡‰è©²åªæœ‰ä¸€å€‹ 1"
    assert submission_df['new_filename'].duplicated().sum() == 0, "æª”åä¸æ‡‰é‡è¤‡"
    print("âœ“ æ ¼å¼é©—è­‰é€šéï¼")
    
    return submission_df